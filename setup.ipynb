{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from matplotlib import image\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions_train.json', mode = 'r') as f:\n",
    "    captions_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = defaultdict(list)\n",
    "for annot in captions_data['annotations']:\n",
    "    captions_dict[annot['image_id']].append(annot['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process captions to calculate inverse frequencies\n",
    "frequencies = defaultdict(lambda: 0)\n",
    "total_count = 0\n",
    "for id in captions_dict:\n",
    "    for caption in captions_dict[id]:\n",
    "        for word in caption.split():\n",
    "            frequencies[word] += 1\n",
    "            total_count += 1\n",
    "inv_frequencies = dict()\n",
    "for word in frequencies.keys():\n",
    "    inv_frequencies[word] = total_count / frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000000009.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.set_image_backend('accimage')\n",
    "train_dir = 'train_images'\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(train_dir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=(0.5,1.2), saturation=0.5, contrast=(0.2, 2), hue=0.08),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "efficientnet_b6 = models.efficientnet_b6(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bab3bcc10fb4daf3e3f0428a2b3c296eef59ed9157f42dd3480f6eeaabd32d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
