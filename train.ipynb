{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# need this library to handle pickling bc Colab doesn't support protocol 5 \n",
        "# which we used when pickling\n",
        "!pip install pickle5"
      ],
      "metadata": {
        "id": "3AtrgotAzsHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lObmzI28Lz19"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import pickle5 as pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add to console to prevent runtime from disconnecting\n",
        "```\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\"); \n",
        "    document.querySelector(\"#ok\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ],
      "metadata": {
        "id": "P2kVP_4KSbMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data from Google Cloud"
      ],
      "metadata": {
        "id": "diesK57hR8uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login --no-launch-browser"
      ],
      "metadata": {
        "id": "mphwD84_L3EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train_arrays\n",
        "!mkdir test_arrays\n",
        "!mkdir train_images\n",
        "!mkdir test_images"
      ],
      "metadata": {
        "id": "SWDmJ7dwRmaP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m rsync gs://sai_data/train_images train_images\n",
        "!gsutil -m rsync gs://sai_data/test_images test_images\n",
        "!gsutil -m rsync gs://sai_data/train_arrays train_arrays\n",
        "!gsutil -m rsync gs://sai_data/test_arrays test_arrays"
      ],
      "metadata": {
        "id": "o-wO2Wk_RTe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veN8OgOiLz2D"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5WYZv8DGLz2C"
      },
      "outputs": [],
      "source": [
        "# load model and move it to GPU\n",
        "gpu = torch.device('cuda')\n",
        "cpu = torch.device('cpu')\n",
        "efficientnet_b6 = models.efficientnet_b6(pretrained = True)\n",
        "efficientnet_b6.eval()\n",
        "efficientnet_b6.to(gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CzNDyezBLz2F"
      },
      "outputs": [],
      "source": [
        "# use optimized image loader\n",
        "torchvision.set_image_backend('accimage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2QRnNDNwLz2F"
      },
      "outputs": [],
      "source": [
        "# normalize using the convention for all pretrained torchvision classifications models\n",
        "normalize = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x.float()),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# apply some data augmenting/model resiliency techniques and then normalize\n",
        "augment_and_normalize = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness = (0.5,1.2), saturation = 0.5, contrast = (0.2, 2), hue = 0.08),\n",
        "    normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t1CqX_tqLz2G"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_dir, caption_array_dir, id_list, transform = None):\n",
        "        # assumes that captions are downloaded as jpgs (with no extra processing)\n",
        "        # and saved in the folder img_dir\n",
        "        self.img_dir = img_dir\n",
        "        # assumes that captions are already preprocessed and represented as numpy arrays in\n",
        "        # the folder caption_array_dir\n",
        "        self.caption_array_dir = caption_array_dir\n",
        "        # list of image ids used for both images and caption arrays\n",
        "        self.id_list = id_list\n",
        "        self.transform = transform if transform else normalize\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(filename[-4:] == '.jpg' for filename in os.listdir(self.img_dir))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.id_list[index]\n",
        "        # filenames are of the form id.jpg where the id is padded with zeroes to the left\n",
        "        # until it has length 12\n",
        "        filename = str(index).zfill(12) + '.jpg'\n",
        "        # each image comes with at least 5 captions, so choose one at random\n",
        "        # caption arrays have format id_n.jpg where id is not padded with zeroes\n",
        "        # and n is an integer between 0 and 4 indicating which of the 5 captions is represented\n",
        "        i = randint(0, 4)\n",
        "        with open(f\"{self.caption_array_dir}/{index}_{i}.npy\", mode = \"rb\") as f:\n",
        "            arr = np.load(f)\n",
        "        img = torchvision.io.read_image(f\"{self.img_dir}/{filename}\")\n",
        "        # convert to RGB if grayscale\n",
        "        if img.shape[0] == 1:\n",
        "            img = img.repeat(3, 1, 1)\n",
        "        elif img.shape[0] != 3:\n",
        "            print(\"improper shape: \", tuple(img.shape))\n",
        "            return\n",
        "        # apply transform for images and just create an equivalent tensor for caption array\n",
        "        return torch.from_numpy(arr), self.transform(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xRKqI14_Lz2I"
      },
      "outputs": [],
      "source": [
        "train_image_dir = 'train_images'\n",
        "train_caption_dir = 'train_arrays'\n",
        "# retrieve saved id list\n",
        "with open(\"train_ids.pkl\", mode = \"rb\") as f:\n",
        "    train_ids = pickle.load(f)\n",
        "train_dataset = ImageCaptionDataset(train_image_dir, train_caption_dir, train_ids, transform = augment_and_normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPzs1qTGLz2K"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "num_workers = 5\n",
        "# pads tensor by adding zeroes at top to extend tensor length to length\n",
        "# this is necessary since the sequences have different lengths\n",
        "def pad_top(tensor, length):\n",
        "    m, n = tensor.shape\n",
        "    assert length >= m, f\"tensor is already too long: {m} > {length}\"\n",
        "    return torch.cat((torch.zeros(length - m, n), tensor))\n",
        "\n",
        "# stacks images and caption arrays after padding arrays to make all of them the same length\n",
        "def custom_collate(batch):\n",
        "    captions, imgs = list(zip(*batch))\n",
        "    imgs = torch.stack(imgs)\n",
        "    length = max(caption.shape[0] for caption in captions)\n",
        "    captions = torch.stack([pad_top(caption, length) for caption in captions])\n",
        "    return captions, imgs\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn = custom_collate, batch_size = batch_size,\n",
        "                              shuffle = True, num_workers = num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qdSkysMYLz2L"
      },
      "outputs": [],
      "source": [
        "test_image_dir = 'test_images'\n",
        "test_caption_dir = 'test_arrays'\n",
        "# retrieve saved id list\n",
        "with open(\"test_ids.pkl\", mode = \"rb\") as f:\n",
        "    test_ids = pickle.load(f)\n",
        "test_dataset = ImageCaptionDataset(test_image_dir, test_caption_dir, test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZEix5rLLz2M"
      },
      "outputs": [],
      "source": [
        "test_batch_size = batch_size\n",
        "test_num_workers = num_workers\n",
        "# no point in shuffling since we're not calculating batch gradients for test data\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn = custom_collate, batch_size = test_batch_size,\n",
        "                             num_workers = test_num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkC3KLlhLz2N"
      },
      "outputs": [],
      "source": [
        "train_iter = iter(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_batches = []\n",
        "for _ in range(3):\n",
        "    print(\"before: \", torch.cuda.memory_allocated())\n",
        "    # use no_grad to let gpu free memory that's no longer being used\n",
        "    # otherwise it holds onto the memory to compute gradients\n",
        "    with torch.no_grad():\n",
        "        class_batches.append(efficientnet_b6.forward(next(train_iter)[1].to(gpu)).to(cpu))\n",
        "    print(\"after: \", torch.cuda.memory_allocated())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3odxiAIurAxC",
        "outputId": "641106a0-c072-44d0-bba5-fd9943d67804"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before:  173611008\n",
            "after:  173611008\n",
            "before:  173611008\n",
            "after:  173611008\n",
            "before:  173611008\n",
            "after:  173611008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GYIZqc8BsNWU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dw1CbqTwJT86"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aIq4Eb4PKd5N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CruyOinGKZdc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "77Oyvz4lKb-b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4-Ozrhz5KrKX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a46mGzVMLFX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jt75kd2aLSCZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iEZ3qRZdMM0S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hE2mIAyOf-en"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8bab3bcc10fb4daf3e3f0428a2b3c296eef59ed9157f42dd3480f6eeaabd32d0"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "89ad9173206cf9d9ef8b207d4848dda7f3f7471d9266639ce7846a81418ce87b"
      }
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}