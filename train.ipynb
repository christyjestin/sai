{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# need this library to handle pickling bc Colab doesn't support protocol 5 which we used when pickling\n",
        "!pip install pickle5"
      ],
      "metadata": {
        "id": "3AtrgotAzsHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lObmzI28Lz19"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms, models\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import pickle5 as pickle\n",
        "from itertools import chain\n",
        "from math import ceil\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add to console to prevent runtime from disconnecting\n",
        "```\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicked to stay connected\"); \n",
        "    document.getElementById(\"header-background\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ],
      "metadata": {
        "id": "P2kVP_4KSbMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data from Google Cloud"
      ],
      "metadata": {
        "id": "diesK57hR8uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login --no-launch-browser"
      ],
      "metadata": {
        "id": "mphwD84_L3EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train_arrays\n",
        "!mkdir test_arrays\n",
        "!mkdir train_images\n",
        "!mkdir test_images"
      ],
      "metadata": {
        "id": "SWDmJ7dwRmaP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m rsync gs://sai_training/train_images train_images\n",
        "!gsutil -m rsync gs://sai_training/test_images test_images\n",
        "!gsutil -m rsync gs://sai_training/train_arrays train_arrays\n",
        "!gsutil -m rsync gs://sai_training/test_arrays test_arrays"
      ],
      "metadata": {
        "id": "o-wO2Wk_RTe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veN8OgOiLz2D"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5WYZv8DGLz2C"
      },
      "outputs": [],
      "source": [
        "# load model and move it to GPU\n",
        "gpu = torch.device('cuda')\n",
        "cpu = torch.device('cpu')\n",
        "efficientnet_b6 = models.efficientnet_b6(pretrained = True)\n",
        "efficientnet_b6.eval()\n",
        "efficientnet_b6.to(gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CzNDyezBLz2F"
      },
      "outputs": [],
      "source": [
        "# use optimized image loader\n",
        "torchvision.set_image_backend('accimage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2QRnNDNwLz2F"
      },
      "outputs": [],
      "source": [
        "# normalize using the convention for all pretrained torchvision classifications models\n",
        "normalize = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x.float()),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# apply some data augmenting/model resiliency techniques and then normalize\n",
        "augment_and_normalize = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness = (0.5,1.2), saturation = 0.5, contrast = (0.2, 2), hue = 0.08),\n",
        "    normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t1CqX_tqLz2G"
      },
      "outputs": [],
      "source": [
        "num_captions_per_image = 5\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_dir, caption_array_dir, id_list, transform = None):\n",
        "        # assumes that captions are downloaded as jpgs (with no extra processing) and saved in the folder img_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.num_images = sum(filename[-4:] == '.jpg' for filename in os.listdir(self.img_dir))\n",
        "        self.num_captions = num_captions_per_image * self.num_images\n",
        "        # assumes that captions are already preprocessed and represented as numpy arrays in the folder caption_array_dir\n",
        "        self.caption_array_dir = caption_array_dir\n",
        "        # list of image ids used for both images and caption arrays\n",
        "        self.id_list = id_list\n",
        "        self.transform = transform if transform else normalize\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_captions\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # find corresponding image and caption indices\n",
        "        i = index % num_captions_per_image\n",
        "        index = index / self.num_images\n",
        "        # find image id\n",
        "        index = self.id_list[index]\n",
        "        # filenames are of the form id.jpg where the id is padded with zeroes to the left until it has length 12\n",
        "        filename = str(index).zfill(12) + '.jpg'\n",
        "        # caption arrays have format id_i.jpg where id is not padded with zeroes\n",
        "        with open(f\"{self.caption_array_dir}/{index}_{i}.npy\", mode = \"rb\") as f:\n",
        "            arr = np.load(f)\n",
        "        img = torchvision.io.read_image(f\"{self.img_dir}/{filename}\")\n",
        "        # convert to RGB if grayscale\n",
        "        if img.shape[0] == 1:\n",
        "            img = img.repeat(3, 1, 1)\n",
        "        elif img.shape[0] != 3:\n",
        "            print(\"improper shape: \", tuple(img.shape))\n",
        "            return\n",
        "        # apply transform for images and just create an equivalent tensor for caption array\n",
        "        return torch.from_numpy(arr).float(), self.transform(img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimilarSizeBatchSampler(Sampler):\n",
        "    def __init__(self, caption_lengths, batch_size):\n",
        "        self.length = ceil(len(caption_lengths) / batch_size)\n",
        "        pairs = list(enumerate(caption_lengths))\n",
        "        # shuffle first so that the sorting doesn't produce the same ordering every single time\n",
        "        shuffle(pairs)\n",
        "        reordered_pairs = sorted(pairs, key = lambda x : x[1])\n",
        "        # grab index i.e. the first value from every pair\n",
        "        reordered_indices = list(zip(*reordered_pairs))[0]\n",
        "        reordered = list(reordered_indices)\n",
        "        batches = []\n",
        "        for i in range(self.length):\n",
        "            batches.append(reordered[i * batch_size : (i + 1) * batch_size])\n",
        "        # each batch contains samples that are similar in length, but we want to change up the order\n",
        "        # for the different sized batches in order to make the mini batch gradient \"more stochastic\"\n",
        "        # while still having each mini batch be made up of samples with similar length \n",
        "        shuffle(batches)\n",
        "        self.batches = batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "metadata": {
        "id": "ZhziLZDUEQdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xRKqI14_Lz2I"
      },
      "outputs": [],
      "source": [
        "train_image_dir = 'train_images'\n",
        "train_caption_dir = 'train_arrays'\n",
        "# retrieve saved id list\n",
        "with open(\"train_ids.pkl\", mode = \"rb\") as f:\n",
        "    train_ids = pickle.load(f)\n",
        "train_dataset = ImageCaptionDataset(train_image_dir, train_caption_dir, train_ids, transform = augment_and_normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPzs1qTGLz2K"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "num_workers = 5\n",
        "# retrieve saved caption length list\n",
        "with open(\"train_caption_lengths.pkl\", mode = \"rb\") as f:\n",
        "    train_caption_lengths = pickle.load(f)\n",
        "\n",
        "# pads tensor by adding zeroes at top to extend tensor length to length\n",
        "# this is necessary since the sequences have different lengths\n",
        "def pad_top(tensor, length):\n",
        "    m, n = tensor.shape\n",
        "    assert length >= m, f\"tensor is already too long: {m} > {length}\"\n",
        "    return torch.cat((torch.zeros(length - m, n), tensor))\n",
        "\n",
        "# stacks images and caption arrays after padding arrays to make all of them the same length\n",
        "def custom_collate(batch):\n",
        "    captions, imgs = list(zip(*batch))\n",
        "    imgs = torch.stack(imgs)\n",
        "    length = max(caption.shape[0] for caption in captions)\n",
        "    captions = torch.stack([pad_top(caption, length) for caption in captions])\n",
        "    return captions, imgs\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn = custom_collate, num_workers = num_workers,\n",
        "                              batch_sampler = SimilarSizeBatchSampler(train_caption_lengths, batch_size))\n",
        "train_iter = iter(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qdSkysMYLz2L"
      },
      "outputs": [],
      "source": [
        "test_image_dir = 'test_images'\n",
        "test_caption_dir = 'test_arrays'\n",
        "# retrieve saved id list\n",
        "with open(\"test_ids.pkl\", mode = \"rb\") as f:\n",
        "    test_ids = pickle.load(f)\n",
        "test_dataset = ImageCaptionDataset(test_image_dir, test_caption_dir, test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZEix5rLLz2M"
      },
      "outputs": [],
      "source": [
        "test_batch_size = batch_size\n",
        "test_num_workers = num_workers\n",
        "# retrieve saved caption length list\n",
        "with open(\"test_caption_lengths.pkl\", mode = \"rb\") as f:\n",
        "    test_caption_lengths = pickle.load(f)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn = custom_collate, num_workers = test_num_workers,\n",
        "                             batch_sampler = SimilarSizeBatchSampler(test_caption_lengths, test_batch_size))\n",
        "test_iter = iter(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_dim = 301\n",
        "hidden_dim = 1200\n",
        "num_layers = 1\n",
        "output_dim = 1000\n",
        "lstm = nn.LSTM(input_size = word_dim, hidden_size = hidden_dim, num_layers = num_layers, batch_first = True, proj_size = output_dim)\n",
        "seq = nn.Sequential(\n",
        "    nn.GELU(),\n",
        "    nn.Linear(1000, 1000),\n",
        "    nn.Softmax(dim = 1),\n",
        ")\n",
        "\n",
        "lstm.to(gpu)\n",
        "seq.to(gpu)\n",
        "\n",
        "def model(batch):\n",
        "    return seq(lstm(batch)[0])\n",
        "\n",
        "cos_sim = nn.CosineSimilarity(dim = 1)\n",
        "optimizer = optim.Adam(chain(lstm.parameters(), seq.parameters()), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "GYIZqc8BsNWU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses= []\n",
        "train_length = len(train_ids)\n",
        "test_length = len(test_ids)\n",
        "\n",
        "for epoch in tqdm(range(2000), desc=\"Epoch\"):\n",
        "    total_loss = 0\n",
        "    for train_inputs, train_labels in train_iter:\n",
        "        n = train_inputs.shape[0]\n",
        "        train_inputs, train_labels = train_inputs.to(gpu), train_labels.to(gpu)\n",
        "        with torch.no_grad():\n",
        "            train_inputs = efficientnet_b6.forward(train_inputs)\n",
        "        train_outputs = model(train_inputs)\n",
        "        loss = torch.mean(cos_sim(train_outputs, train_labels))\n",
        "        with torch.no_grad():\n",
        "            total_loss += loss.item() * n\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    train_losses.append(total_loss / train_length)\n",
        "\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for test_inputs, test_labels in test_iter:\n",
        "            n = test_inputs.shape[0]\n",
        "            test_inputs, test_labels = test_inputs.to(gpu), test_labels.to(gpu)\n",
        "            test_inputs = efficientnet_b6.forward(test_inputs)\n",
        "            test_outputs = model(test_inputs)\n",
        "            test_loss += torch.sum(cos_sim(test_outputs, test_labels)).item()\n",
        "        test_losses.append(test_loss / test_length)"
      ],
      "metadata": {
        "id": "dw1CbqTwJT86"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(test_losses, label=\"Test Loss\")\n",
        "plt.set_title(\"Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aIq4Eb4PKd5N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CruyOinGKZdc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "77Oyvz4lKb-b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4-Ozrhz5KrKX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a46mGzVMLFX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jt75kd2aLSCZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iEZ3qRZdMM0S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hE2mIAyOf-en"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8bab3bcc10fb4daf3e3f0428a2b3c296eef59ed9157f42dd3480f6eeaabd32d0"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "89ad9173206cf9d9ef8b207d4848dda7f3f7471d9266639ce7846a81418ce87b"
      }
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}